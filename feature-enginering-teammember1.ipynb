{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10151,"databundleVersionId":59042,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# First clone the repository\n!git clone https://github.com/BloodAxe/Kaggle-Salt.git /kaggle/working/Kaggle-Salt\n\nimport sys\nsys.path.append(\"/kaggle/working/Kaggle-Salt\")\n\n# standard libraries\nimport os\nfrom typing import Optional\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport json\nimport random\nfrom lib.dataset import medium_augmentations, drop_some, normalize_image\nimport cv2\nfrom albumentations.pytorch import ToTensorV2\n\n# data processing libraries\nimport numpy as np\nimport pandas as pd\nfrom skimage.morphology import remove_small_objects, remove_small_holes\nfrom sklearn.utils import check_random_state\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch and related\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.backends import cudnn\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import make_grid\n\n# Albumentations for augmentations\nimport albumentations as A\nimport albumentations.augmentations.functional as AF\n\n# progress bar\nfrom tqdm import tqdm\n\n#custom modules from the cloned repo\ntry:\n    from models.modules.abn import ABN, ACT_RELU\n    from models.resnext import try_index\n    import lib.augmentations as AA\n    from lib import torch_augmentation_functional as TAF\n    from lib import train_utils as U\n    from lib.common import find_in_dir, is_sorted\n    from nnn import ssim_cv\nexcept ImportError as e:\n    print(f\"Error importing custom modules: {e}\")\n    print(\"Please ensure the repository is properly cloned and paths are set\")\n\n#augmentation pipeline\nbase_aug = medium_augmentations()\n\n#Define a standalone z-score normalize function\ndef zscore_image(image, **kwargs):\n    return normalize_image(image)\n\n#rain_transform\ntrain_transform = A.Compose([\n    # per-image z-score normalization\n    A.Lambda(image=zscore_image),\n    # medium-strength augmentations\n    *base_aug.transforms,\n    # final normalize + to-tensor\n    A.Normalize(mean=0.5, std=0.224, max_pixel_value=255.0),\n    ToTensorV2()\n])\nprint(\"Contents of input directory:\")\nprint(os.listdir('/kaggle/input/tgs-salt-identification-challenge'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:57:23.934740Z","iopub.execute_input":"2025-04-28T20:57:23.935682Z","iopub.status.idle":"2025-04-28T20:57:24.116570Z","shell.execute_reply.started":"2025-04-28T20:57:23.935647Z","shell.execute_reply":"2025-04-28T20:57:24.115492Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path '/kaggle/working/Kaggle-Salt' already exists and is not an empty directory.\nError importing custom modules: cannot import name 'torch_augmentation_functional' from 'lib' (unknown location)\nPlease ensure the repository is properly cloned and paths are set\nContents of input directory:\n['depths.csv', 'sample_submission.csv', 'train.zip', 'competition_data.zip', 'test.zip', 'train.csv', 'flamingo.zip']\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.937125Z","iopub.status.idle":"2025-04-28T20:06:45.937446Z","shell.execute_reply.started":"2025-04-28T20:06:45.937307Z","shell.execute_reply":"2025-04-28T20:06:45.937320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.938475Z","iopub.status.idle":"2025-04-28T20:06:45.938743Z","shell.execute_reply.started":"2025-04-28T20:06:45.938618Z","shell.execute_reply":"2025-04-28T20:06:45.938631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_ROOT = 'data'\nN_FOLDS = 5\nORIGINAL_SIZE = 101\n\n\ndef all_train_ids() -> np.ndarray:\n    \"\"\"\n    Return all train ids\n    :return: Numpy array of ids\n    \"\"\"\n    return np.array(sorted([id_from_fname(fname) for fname in find_in_dir(os.path.join(DATA_ROOT, 'train', 'images'))]))\n\n\ndef all_test_ids() -> np.ndarray:\n    \"\"\"\n    Return all test ids\n    :return: Numpy array of ids\n    \"\"\"\n    return np.array(sorted([id_from_fname(fname) for fname in find_in_dir(os.path.join(DATA_ROOT, 'test', 'images'))]))\n\n\ndef read_train_image(sample_id) -> np.ndarray:\n    return cv2.imread(os.path.join(DATA_ROOT, 'train', 'images', '%s.png' % sample_id), cv2.IMREAD_GRAYSCALE)\n\n\ndef read_test_image(sample_id) -> np.ndarray:\n    return cv2.imread(os.path.join(DATA_ROOT, 'test', 'images', '%s.png' % sample_id), cv2.IMREAD_GRAYSCALE)\n\n\ndef read_train_mask(sample_id) -> np.ndarray:\n    mask = cv2.imread(os.path.join(DATA_ROOT, 'train', 'masks', '%s.png' % sample_id), cv2.IMREAD_GRAYSCALE)\n    mask = (mask > 0).astype(np.uint8)\n    return mask\n\n\ndef read_train_images(ids) -> np.ndarray:\n    \"\"\"\n    Reads train images. Returns numpy array of shape [N;H;W], where N is number of images, H - height, W - width.\n    Images read as np.uint8 type with range in [0.255]\n    :param ids: List of image ids.\n    :return: Numpy array\n    \"\"\"\n    if not is_sorted(ids):\n        raise ValueError('Array ids must be sorted')\n\n    images = [read_train_image(sample_id) for sample_id in ids]\n    images = np.array(images, dtype=np.uint8)\n    return images\n\n\ndef read_test_images(ids) -> np.ndarray:\n    \"\"\"\n    Reads test images. Returns numpy array of shape [N;H;W], where N is number of images, H - height, W - width\n    Images read as np.uint8 type with range in [0.255]\n    :param ids: List of image ids.\n    :return: Numpy array\n    \"\"\"\n    if not is_sorted(ids):\n        raise ValueError('Array ids must be sorted')\n\n    images = [read_test_image(sample_id) for sample_id in ids]\n    images = np.array(images, dtype=np.uint8)\n    return images\n\n\ndef read_train_masks(ids) -> np.ndarray:\n    \"\"\"\n    Reads train masks. Returns numpy array of shape [N;H;W], where N is number of images, H - height, W - width\n    :param ids: List of image ids.\n    :return: Numpy array with values {0,1}\n    \"\"\"\n    if not is_sorted(ids):\n        raise ValueError('Array ids must be sorted')\n\n    images = [read_train_mask(sample_id) for sample_id in ids]\n    images = np.array(images, dtype=np.uint8)\n    return images\n\n\ndef read_depths(ids):\n    if not is_sorted(ids):\n        raise ValueError('Array ids must be sorted')\n\n    df = pd.read_csv(os.path.join(DATA_ROOT, 'depths.csv'))\n    df['z'] = df['z'].astype(np.float32)\n    df['z'] = df['z'] / df['z'].max()\n\n    depths = []\n    for sample_id in ids:\n        z = df[df['id'] == sample_id].iloc[0]['z']\n        depths.append(z)\n    return np.array(depths)\n\n\ndef get_selection_mask(ids: np.ndarray, query: np.ndarray):\n    if not is_sorted(ids):\n        raise ValueError('Array ids must be sorted')\n\n    if not is_sorted(query):\n        raise ValueError('Array subset must be sorted')\n\n    if not np.in1d(query, ids, assume_unique=True).all():\n        raise ValueError(\"Some elements of subset are not in ids\")\n\n    # mask2 = []\n    # for sample_id in subset:\n    #     index = np.argwhere(ids == sample_id)[0, 0]\n    #     mask2.append(index)\n    # mask2 = np.array(mask2)\n    # return mask2\n\n    mask = np.array([sample_id in query for sample_id in ids])\n    return mask\n\n\ndef drop_some(images, masks, drop_black=True, drop_vstrips=False, drop_empty=False, drop_few=None) -> np.ndarray:\n    skips = []\n\n    dropped_blacks = 0\n    dropped_vstrips = 0\n    dropped_few = 0\n    dropped_empty = 0\n\n    for image, mask in zip(images, masks):\n        should_keep = True\n\n        if drop_black and is_black(image, mask):\n            should_keep = False\n            dropped_blacks += 1\n\n        if drop_vstrips and is_vertical_strips(image, mask):\n            should_keep = False\n            dropped_vstrips += 1\n\n        if drop_few and is_salt_less_than(image, mask, int(drop_few)) and not is_salt_less_than(image, mask, 1):\n            should_keep = False\n            dropped_few += 1\n\n        if drop_empty and is_salt_less_than(image, mask, 1):\n            should_keep = False\n            dropped_empty += 1\n\n        skips.append(should_keep)\n\n    print(f'Dropped {dropped_blacks} black images; {dropped_vstrips} vertical strips; {dropped_empty}  empty masks; {dropped_few} few-pixel salt')\n    return np.array(skips)\n\n\ndef cumsum(img, axis=0) -> np.ndarray:\n    \"\"\"\n    https://www.kaggle.com/bguberfain/unet-with-depth#360485\n    For what I know about seismic imaging, the cumsum on the depth axis will (remotely) approximate an inversion operation* (that is, convert from interface transition to interface properly).\n    :param axis:\n    :param img: Single-channel image\n    :return:\n    \"\"\"\n    x_mean = img.mean()\n    x_csum = (np.float32(img) - x_mean).cumsum(axis=axis)\n    x_csum -= x_csum.mean()\n    x_csum /= max(1e-3, x_csum.std())\n    return x_csum\n\n\ndef id_from_fname(fname):\n    return os.path.splitext(os.path.basename(fname))[0]\n\n\ndef harder_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    border_mode = U.get_border_mode(border_mode)\n    aug = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightness(p=0.5),\n        A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n        A.OneOf([A.IAAAdditiveGaussianNoise(), A.GaussNoise()], p=0.3),\n        A.OneOf([A.MotionBlur(p=.1), A.MedianBlur(blur_limit=3, p=0.1), A.Blur(blur_limit=3, p=0.1)], p=0.2),\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5, border_mode=border_mode),\n            A.RandomSizedCrop((int(target_size * 0.8), target_size), target_size, target_size, p=0.5),\n        ], p=0.5),\n        A.OneOf([\n            AA.AxisShear(sx=0.1, sy=0.1, border_mode=border_mode, p=0.5),\n            A.ElasticTransform(alpha=0.5, sigma=0.5, alpha_affine=10, border_mode=border_mode),\n            A.GridDistortion(border_mode=border_mode),\n            A.IAAPerspective(p=0.3),\n        ], p=0.3),\n        A.Cutout()\n    ])\n    return aug\n\n\ndef hard_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    border_mode = U.get_border_mode(border_mode)\n    aug = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.OneOf([A.RandomBrightness(p=0.5), A.RandomGamma(gamma_limit=(80, 120), p=0.5)]),\n        A.OneOf([A.IAAAdditiveGaussianNoise(), A.GaussNoise()], p=0.3),\n        A.OneOf([A.MotionBlur(p=.1), A.MedianBlur(blur_limit=3, p=0.1), A.Blur(blur_limit=3, p=0.1)], p=0.2),\n        A.ShiftScaleRotate(shift_limit=0.075, scale_limit=0.075, rotate_limit=10, p=0.5, border_mode=border_mode),\n        A.OneOf([\n            A.ElasticTransform(alpha=0.5, sigma=0.5, alpha_affine=10, border_mode=border_mode, p=0.1),\n            A.GridDistortion(border_mode=border_mode, p=0.1),\n        ], p=0.3),\n        A.Cutout()\n    ])\n\n    return aug\n\n\ndef medium_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    border_mode = U.get_border_mode(border_mode)\n    aug = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightness(p=0.25),\n        A.RandomGamma(gamma_limit=(80, 120), p=0.25),\n        AA.RandomContrastGray(p=0.25),\n        A.ShiftScaleRotate(shift_limit=0.10, scale_limit=0.10, rotate_limit=5, p=0.5, border_mode=border_mode),\n        A.OneOf([\n            A.ElasticTransform(alpha=0.5, sigma=0.5, alpha_affine=10, border_mode=border_mode, p=0.1),\n            A.GridDistortion(border_mode=border_mode, p=0.1),\n            A.NoOp(p=0.5)\n        ]),\n        A.OneOf([A.IAAAdditiveGaussianNoise(), A.GaussNoise()], p=0.3),\n    ])\n    return aug\n\n\ndef light_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    border_mode = U.get_border_mode(border_mode)\n    aug = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.10, scale_limit=0.10, rotate_limit=0, p=0.5, border_mode=border_mode),\n        A.OneOf([\n            A.ElasticTransform(alpha=0.5, sigma=0.5, alpha_affine=10, border_mode=border_mode, p=0.1),\n            A.GridDistortion(border_mode=border_mode, p=0.1),\n            A.NoOp(p=0.5)\n        ]),\n    ])\n    return aug\n\n\ndef flip_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    border_mode = U.get_border_mode(border_mode)\n    aug = A.HorizontalFlip(p=0.5)\n    return aug\n\n\ndef none_augmentations(target_size, border_mode=cv2.BORDER_CONSTANT):\n    return A.NoOp(p=1)\n\n\nclass DatasetResizePad:\n    def __init__(self, resize_size, target_size, border_mode=cv2.BORDER_CONSTANT, border_fill=0, interpolation=cv2.INTER_LINEAR, **kwargs):\n        self.resize_size = resize_size\n        self.target_size = target_size\n        self.border_mode = border_mode\n        self.border_fill = border_fill\n        self.interpolation = interpolation\n\n        self.t_forward = A.Compose([\n            A.Resize(resize_size, resize_size, interpolation=interpolation),\n            A.PadIfNeeded(min_height=target_size, min_width=target_size, border_mode=border_mode)])\n\n    def forward(self, **kwargs):\n        return self.t_forward(**kwargs)\n\n    def backward(self, x):\n        if isinstance(x, torch.Tensor):\n            x = TAF.central_crop(x, self.resize_size, self.resize_size)\n        elif isinstance(x, np.ndarray):\n            x = AF.center_crop(x, self.resize_size, self.resize_size).ascontiguousarray()\n        return x\n\n    def __repr__(self):\n        return f'DatasetResizePad(resize_size={self.resize_size}, target_size={self.target_size}, border_mode={self.border_mode}, border_fill={self.border_fill}, interpolation={self.interpolation})'\n\n\ndef get_prepare_fn(name, border_mode=cv2.BORDER_DEFAULT, **kwargs):\n    border_mode = U.get_border_mode(border_mode)\n    if name is None or name == 'None':\n        return None\n    if name == '128':\n        return DatasetResizePad(resize_size=128, target_size=128, border_mode=border_mode, **kwargs)\n    if name == '224':\n        return DatasetResizePad(resize_size=224, target_size=224, border_mode=border_mode, **kwargs)\n    if name == '256':\n        return DatasetResizePad(resize_size=256, target_size=256, border_mode=border_mode, **kwargs)\n    if name == '128pad':\n        return DatasetResizePad(resize_size=ORIGINAL_SIZE, target_size=128, border_mode=border_mode, **kwargs)\n    if name == '224pad':\n        return DatasetResizePad(resize_size=ORIGINAL_SIZE * 2, target_size=224, border_mode=border_mode, **kwargs)\n    if name == '256pad':\n        return DatasetResizePad(resize_size=ORIGINAL_SIZE * 2, target_size=256, border_mode=border_mode, **kwargs)\n\n    raise ValueError('Unsupported prepare fn')\n\n\nclass ImageAndMaskDataset(Dataset):\n    \"\"\"\n    Creates a dataset object.\n    :param images - List of images\n    :param masks - List of masks\n    :param depths - List of depths\n\n    \"\"\"\n\n    def __init__(self, ids: np.ndarray, images: np.ndarray, masks: Optional[np.ndarray], depths: np.ndarray,\n                 prepare_fn: DatasetResizePad = None,\n                 normalize=A.Normalize(mean=0.5, std=0.224, max_pixel_value=255.0),\n                 augment=None):\n\n        if not is_sorted(ids):\n            raise ValueError('Array ids must be sorted')\n\n        self.ids = ids\n        self.images = images\n        self.masks = masks\n        self.depths = depths\n        self.augment = augment\n        self.normalize = normalize\n        self.num_channels = 1\n        self.resize_fn = prepare_fn\n\n        if prepare_fn is not None:\n            self.images = np.array([self.resize_fn.forward(image=x)['image'] for x in self.images])\n            if self.masks is not None:\n                self.masks = np.array([self.resize_fn.forward(image=x, mask=x)['mask'] for x in self.masks])\n\n    def __getitem__(self, index):\n\n        data = {'image': self.images[index].copy()}\n        if self.masks is not None:\n            data['mask'] = self.masks[index].copy()\n\n        if self.augment is not None:\n            data = self.augment(**data)\n\n        data = self.normalize(**data)\n\n        image = np.expand_dims(data['image'], 0)\n        image = torch.from_numpy(image).float()\n\n        sample = {\n            'index': index,\n            'id': self.ids[index],\n            'image': image,\n            'depth': self.depths[index],\n        }\n\n        mask = data.get('mask', None)\n        if mask is not None:\n            if not np.isin(mask, [0, 1]).all():\n                raise RuntimeError(f'A mask after augmentation contains values other than {{0;1}}: {np.unique(mask)}')\n\n            # BCE problem, so float target\n            mask_class = np.array((mask > 0).any(), dtype=np.float32)\n            mask_class = np.expand_dims(mask_class, 0)\n\n            mask = np.expand_dims(mask, 0)\n            mask = torch.from_numpy(mask).float()\n\n            sample['mask'] = mask\n            sample['class'] = mask_class\n\n        return sample\n\n    def channels(self):\n        return self.num_channels\n\n    def __len__(self):\n        return len(self.images)\n\n\ndef get_folds_vector(kind, images, masks, depths, n_folds=N_FOLDS, random_state=None):\n    n = len(depths)\n    folds = np.array(list(range(n_folds)) * n)[:n]\n\n    rnd = check_random_state(random_state)\n\n    if kind == 'coverage' or kind == 'area':\n        coverage = np.array([cv2.countNonZero(x) for x in masks], dtype=np.int)\n        sorted_indexes = np.argsort(coverage)\n    elif kind == 'depth':\n        sorted_indexes = np.argsort(depths)\n    elif kind == 'resolution':\n        resolution = np.array([cv2.Laplacian(image, cv2.CV_32F, borderType=cv2.BORDER_REFLECT101).std() for image in images])\n        sorted_indexes = np.argsort(resolution)\n    else:\n        sorted_indexes = list(range(n))\n        rnd.shuffle(sorted_indexes)\n\n    return folds[sorted_indexes]\n\n\ndef get_train_test_split_for_fold(stratify, fold, ids):\n    folds = pd.read_csv(os.path.join('data', f'folds_by_{stratify}.csv'))\n    folds = np.array([folds[folds['id'] == id].iloc[0]['fold'] for id in ids])\n    return folds != fold, folds == fold\n\n\ndef fix_mask(mask):\n    \"\"\"\n    Tries to 'fix' a mask by filling gaps and removing single-pixel noise\n    :param mask:\n    :return:\n    \"\"\"\n    mask = mask.astype(np.bool)\n    mask = remove_small_holes(mask, area_threshold=12, connectivity=1)\n    mask = remove_small_objects(mask, min_size=12, connectivity=1)\n\n    # kernel = np.ones((5, 5), dtype=np.uint8)\n    # mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, borderType=cv2.BORDER_REFLECT101)\n    # mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, borderType=cv2.BORDER_REFLECT101)\n    return mask.astype(np.uint8)\n\n\ndef fix_masks(masks, train_ids):\n    changed_masks = []\n    changed_ids = []\n\n    for image_id, mask in zip(train_ids, masks):\n        new_mask = fix_mask(mask)\n        changed_masks.append(new_mask)\n\n        if not np.array_equal(new_mask, mask):\n            changed_ids.append(image_id)\n\n    masks = np.array(changed_masks)\n    return masks, changed_ids\n\n\ndef is_black(image, mask):\n    return image.sum() == 0\n\n\ndef is_vertical_strips(image, mask):\n    colsum = np.sum(mask, axis=0)\n    uniq = np.unique(colsum)\n    return len(uniq) == 2 and uniq.min() == 0 and uniq.max() == mask.shape[0]\n\n\ndef is_salt_less_than(image, mask, threshold):\n    return mask.sum() < threshold\n\n\ndef is_salt_greater_than(image, mask, threshold):\n    return mask.sum() > threshold\n\n\nAUGMENTATION_MODES = {\n    'harder': harder_augmentations,\n    'hard': hard_augmentations,\n    'medium': medium_augmentations,\n    'light': light_augmentations,\n    'flip': flip_augmentations,\n    'none': none_augmentations,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.941015Z","iopub.status.idle":"2025-04-28T20:06:45.941294Z","shell.execute_reply.started":"2025-04-28T20:06:45.941169Z","shell.execute_reply":"2025-04-28T20:06:45.941181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n@clipped\ndef random_contrast_gray(img, alpha):\n    gray = ((1.0 - alpha) / img.size) * np.sum(img)\n    return alpha * img + gray\n\n\nclass RandomContrastGray(A.ImageOnlyTransform):\n    \"\"\"Randomly change contrast of the input image.\n\n    Args:\n        limit ((float, float) or float): factor range for changing contrast. If limit is a single float, the range\n            will be (-limit, limit). Default: 0.2.\n        p (float): probability of applying the transform. Default: 0.5.\n\n    Targets:\n        image\n\n    Image types:\n        uint8, float32\n    \"\"\"\n\n    def __init__(self, limit=.2, p=.5):\n        super(RandomContrastGray, self).__init__(p)\n        self.limit = A.to_tuple(limit)\n\n    def apply(self, img, alpha=0.2, **params):\n        return random_contrast_gray(img, alpha)\n\n    def get_params(self):\n        return {'alpha': 1.0 + random.uniform(self.limit[0], self.limit[1])}\n\n\nclass AxisShear(A.DualTransform):\n    def __init__(self, sx=0.1, sy=0.1, p=0.5, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT):\n        super(AxisShear, self).__init__(p)\n        self.sx = sx\n        self.sy = sy\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n\n    def get_params(self):\n        return {\"cx\": random.uniform(0, 1),\n                \"cy\": random.uniform(0, 1),\n                \"sx\": random.uniform(- self.sx, self.sx),\n                \"sy\": random.uniform(- self.sx, self.sx)}\n\n    def apply(self, img, cx=0.5, cy=0.5, sx=0, sy=0, interpolation=cv2.INTER_LINEAR, **params):\n        center = np.eye(3, 3)\n        center[0, 2] = cx * img.shape[1]\n        center[1, 2] = cy * img.shape[0]\n\n        inv_center = np.eye(3, 3)\n        inv_center[0, 2] = -center[0, 2]\n        inv_center[1, 2] = -center[1, 2]\n\n        shear = np.eye(3, 3)\n        shear[0, 1] = sx\n        shear[1, 0] = sy\n\n        m = np.matmul(np.matmul(center, shear), inv_center)\n        return cv2.warpAffine(img, m[:2, ...], dsize=(img.shape[1], img.shape[0]), flags=interpolation, borderMode=self.border_mode)\n\n\nclass AxisScale(A.DualTransform):\n    def __init__(self, sx=0.1, sy=0.1, p=0.5, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT):\n        super(AxisScale, self).__init__(p)\n        self.sx = sx\n        self.sy = sy\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n\n    def get_params(self):\n        return {\"cx\": random.uniform(0, 1),\n                \"cy\": random.uniform(0, 1),\n                \"sx\": random.uniform(1 - self.sx, 1 + self.sx),\n                \"sy\": random.uniform(1 - self.sx, 1 + self.sx)}\n\n    def apply(self, img, cx=0.5, cy=0.5, sx=1, sy=1, interpolation=cv2.INTER_LINEAR, **params):\n        center = np.eye(3, 3)\n        center[0, 2] = cx * img.shape[1]\n        center[1, 2] = cy * img.shape[0]\n\n        inv_center = np.eye(3, 3)\n        inv_center[0, 2] = -center[0, 2]\n        inv_center[1, 2] = -center[1, 2]\n\n        scale = np.eye(3, 3)\n        scale[0, 0] = sx\n        scale[1, 1] = sy\n\n        m = np.matmul(np.matmul(center, scale), inv_center)\n        return cv2.warpAffine(img, m[:2, ...], dsize=(img.shape[1], img.shape[0]), flags=interpolation, borderMode=self.border_mode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.942618Z","iopub.status.idle":"2025-04-28T20:06:45.942911Z","shell.execute_reply.started":"2025-04-28T20:06:45.942757Z","shell.execute_reply":"2025-04-28T20:06:45.942769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef test_ssim_normalization():\n    assert ssim_cv(np.zeros((101, 101), dtype=np.uint8),\n                   np.zeros((101, 101), dtype=np.uint8)) == 1.0\n\n    assert ssim_cv(np.ones((101, 101), dtype=np.uint8) * 255,\n                   np.ones((101, 101), dtype=np.uint8) * 255) == 1.0\n\n    assert ssim_cv(np.zeros((101, 101), dtype=np.uint8),\n                   np.ones((101, 101), dtype=np.uint8) * 255) < 0.0001\n\n    assert ssim_cv(np.ones((101, 101), dtype=np.uint8) * 127,\n                   np.ones((101, 101), dtype=np.uint8) * 255) > 0.5\n\n    one_black = np.ones((101, 101), dtype=np.uint8) * 255\n    one_black[1, 1] = 0\n\n    assert ssim_cv(one_black,\n                   np.ones((101, 101), dtype=np.uint8) * 255) > 0.99","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.944158Z","iopub.status.idle":"2025-04-28T20:06:45.944498Z","shell.execute_reply.started":"2025-04-28T20:06:45.944318Z","shell.execute_reply":"2025-04-28T20:06:45.944336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import device\n\n# Clone and set up repository\nrepo_path = '/kaggle/working/Kaggle-Salt'\nif not os.path.exists(repo_path):\n    !git clone https://github.com/BloodAxe/Kaggle-Salt.git {repo_path}\n\n# Add to Python path\nsys.path.insert(0, '/kaggle/working')\nsys.path.insert(0, repo_path)\n\n# First try importing the compiled version\ntry:\n    from models.modules.abn import ABN\n    from models.modules.functions import inplace_abn, inplace_abn_sync\n    print(\"Successfully imported compiled ABN modules\")\nexcept ImportError as e:\n    print(f\"Import failed: {e}. Using fallback implementation...\")\n    \n    # Fallback implementation\n    class ABN(nn.Module):\n        def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                    activation=\"leaky_relu\", slope=0.01):\n            super().__init__()\n            self.bn = nn.BatchNorm2d(num_features, eps=eps, momentum=momentum, affine=affine)\n            self.activation = activation\n            self.slope = slope\n            self.eps = eps\n            self.momentum = momentum\n            \n        def forward(self, x):\n            x = self.bn(x)\n            if self.activation == \"leaky_relu\":\n                return F.leaky_relu(x, negative_slope=self.slope)\n            return x\n    \n    def inplace_abn(x, weight, bias, running_mean, running_var, training, momentum, eps, activation, slope):\n        return ABN(x.size(1), eps, momentum, True, activation, slope)(x)\n    \n    inplace_abn_sync = inplace_abn\n\n# Implement InPlaceABN with proper attribute inheritance\nclass InPlaceABN(ABN):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 activation=\"leaky_relu\", slope=0.01):\n        super().__init__(num_features, eps, momentum, affine, activation, slope)\n        \n    def forward(self, x):\n        if hasattr(self, 'bn'):  # Fallback case\n            return super().forward(x)\n        else:  # Compiled case\n            return inplace_abn(\n                x, \n                self.bn.weight, \n                self.bn.bias, \n                self.bn.running_mean, \n                self.bn.running_var,\n                self.training,\n                self.bn.momentum,\n                self.bn.eps,\n                self.activation,\n                self.slope\n            )\n\nclass InPlaceABNSync(ABN):\n    def __init__(self, num_features, devices=None, eps=1e-5, momentum=0.1,\n                 affine=True, activation=\"leaky_relu\", slope=0.01):\n        super().__init__(num_features, eps, momentum, affine, activation, slope)\n        self.devices = devices or [0]\n        \n    def forward(self, x):\n        if hasattr(self, 'bn'):  # Fallback case\n            return super().forward(x)\n        else:  # Compiled case\n            return inplace_abn_sync(\n                x,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                {},\n                self.training,\n                self.bn.momentum,\n                self.bn.eps,\n                self.activation,\n                self.slope\n            )\n\n# Test\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Test both implementations\n    for cls in [InPlaceABN, InPlaceABNSync]:\n        print(f\"\\nTesting {cls.__name__}:\")\n        layer = cls(64).to(device)\n        x = torch.randn(16, 64, 32, 32).to(device)\n        out = layer(x)\n        print(\"Output shape:\", out.shape)\n        print(\"First values:\", out[0, 0, 0, :5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:16:50.407978Z","iopub.execute_input":"2025-04-28T20:16:50.408304Z","iopub.status.idle":"2025-04-28T20:16:50.482766Z","shell.execute_reply.started":"2025-04-28T20:16:50.408279Z","shell.execute_reply":"2025-04-28T20:16:50.481858Z"}},"outputs":[{"name":"stdout","text":"Import failed: /root/.cache/torch_extensions/py311_cu124/inplace_abn/inplace_abn.so: cannot open shared object file: No such file or directory. Using fallback implementation...\nUsing device: cpu\n\nTesting InPlaceABN:\nOutput shape: torch.Size([16, 64, 32, 32])\nFirst values: tensor([-0.0021, -0.0063, -0.0087,  0.1946, -0.0033], grad_fn=<SliceBackward0>)\n\nTesting InPlaceABNSync:\nOutput shape: torch.Size([16, 64, 32, 32])\nFirst values: tensor([ 1.0436, -0.0130,  1.5882,  0.4891, -0.0073], grad_fn=<SliceBackward0>)\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet34\nfrom torchvision.models.resnet import ResNet34_Weights\n\ndef get_my_encoder(pretrained=True):\n    weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n    model = resnet34(weights=weights)\n    # Remove last fully connected layer and avgpool\n    return nn.Sequential(*list(model.children())[:-2])\n\n# Define decoder\ndef get_my_decoder(mid_channels=256):\n    return nn.Sequential(\n        nn.Conv2d(mid_channels, 128, kernel_size=3, padding=1),\n        nn.BatchNorm2d(128),\n        nn.ReLU(inplace=True),\n        nn.Upsample(scale_factor=2),\n        nn.Conv2d(128, 64, kernel_size=3, padding=1),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 1, kernel_size=1)\n    )\n\nclass SaltUNetWithAux(nn.Module):\n    def __init__(self, encoder, decoder, mid_channels):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n        # Auxiliary classifier\n        self.aux_classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(mid_channels, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        features = self.encoder(x)\n        mask_pred = self.decoder(features)\n        aux_pred = self.aux_classifier(features)\n        return mask_pred, aux_pred\n\ndef lovasz(pred, target):\n    # Placeholder - use actual Lovasz implementation if needed\n    return F.binary_cross_entropy_with_logits(pred, target)\n\nencoder = get_my_encoder(pretrained=True)\ndecoder = get_my_decoder()\nmodel = SaltUNetWithAux(encoder, decoder, mid_channels=256)  # Removed .cuda()\n\n# Training setup\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Example training loop\ndef train_epoch(model, trainloader):\n    model.train()\n    for imgs, masks in trainloader:\n        # Removed .cuda() calls\n        imgs, masks = imgs.float(), masks.float()\n        \n        # Forward pass\n        mask_pred, aux_pred = model(imgs)\n        \n        # Losses\n        loss_mask = lovasz(mask_pred.squeeze(1), masks)\n        salt_present = (masks.view(masks.size(0), -1).sum(dim=1) > 0).float()\n        loss_aux = F.binary_cross_entropy(aux_pred.squeeze(1), salt_present)\n        \n        # Combined loss\n        total_loss = loss_mask + 0.5 * loss_aux\n        \n        # Backward pass\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        \n        print(f\"Seg Loss: {loss_mask.item():.4f}, Aux Loss: {loss_aux.item():.4f}\")\n\n#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:16:57.406365Z","iopub.execute_input":"2025-04-28T20:16:57.407164Z","iopub.status.idle":"2025-04-28T20:16:57.998637Z","shell.execute_reply.started":"2025-04-28T20:16:57.407137Z","shell.execute_reply":"2025-04-28T20:16:57.997956Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"\n\nclass _ConvBatchNormReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            relu=True,\n    ):\n        super(_ConvBatchNormReLU, self).__init__()\n        self.add_module(\n            \"conv\",\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n                bias=False,\n            ),\n        )\n        self.add_module(\n            \"bn\",\n            nn.BatchNorm2d(\n                num_features=out_channels, eps=1e-5, momentum=0.999, affine=True\n            ),\n        )\n\n        if relu:\n            self.add_module(\"relu\", nn.ReLU())\n\n    def forward(self, x):\n        return super(_ConvBatchNormReLU, self).forward(x)\n\n\nclass _ASPPModule(nn.Module):\n    \"\"\"Atrous Spatial Pyramid Pooling with image pool\"\"\"\n\n    def __init__(self, in_channels, out_channels, pyramids):\n        super(_ASPPModule, self).__init__()\n        self.stages = nn.Module()\n        self.stages.add_module(\n            \"c0\", _ConvBatchNormReLU(in_channels, out_channels, 1, 1, 0, 1)\n        )\n        for i, (dilation, padding) in enumerate(zip(pyramids, pyramids)):\n            self.stages.add_module(\n                \"c{}\".format(i + 1),\n                _ConvBatchNormReLU(in_channels, out_channels, 3, 1, padding, dilation),\n            )\n        self.imagepool = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"pool\", nn.AdaptiveAvgPool2d(1)),\n                    (\"conv\", _ConvBatchNormReLU(in_channels, out_channels, 1, 1, 0, 1)),\n                ]\n            )\n        )\n\n    def forward(self, x):\n        h = self.imagepool(x)\n        h = [F.interpolate(h, size=x.shape[2:], mode=\"bilinear\")]\n        for stage in self.stages.children():\n            h += [stage(x)]\n        h = torch.cat(h, dim=1)\n        return h\n\n\nclass ASPP(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=256,\n                 dilations=(12, 24, 36),\n                 abn_block=ABN,\n                 activation=ACT_RELU,\n                 pooling_size=None):\n        super(ASPP, self).__init__()\n        self.pooling_size = pooling_size\n\n        self.map_convs = nn.ModuleList([\n            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[0], padding=dilations[0]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[1], padding=dilations[1]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[2], padding=dilations[2])\n        ])\n        self.map_bn = abn_block(hidden_channels * 4, activation=activation)\n\n        self.global_pooling_conv = nn.Conv2d(in_channels, hidden_channels, 1, bias=False)\n        self.global_pooling_bn = abn_block(hidden_channels, activation=activation)\n\n        self.red_conv = nn.Conv2d(hidden_channels * 4, out_channels, 1, bias=False)\n        self.pool_red_conv = nn.Conv2d(hidden_channels, out_channels, 1, bias=False)\n        self.red_bn = abn_block(out_channels, activation=activation)\n\n        self.reset_parameters(self.map_bn.activation, self.map_bn.slope)\n\n    def reset_parameters(self, activation, slope):\n        gain = nn.init.calculate_gain(activation, slope)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight.data, gain)\n                if hasattr(m, \"bias\") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, ABN):\n                if hasattr(m, \"weight\") and m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if hasattr(m, \"bias\") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Map convolutions\n        out = torch.cat([m(x) for m in self.map_convs], dim=1)\n        out = self.map_bn(out)\n        out = self.red_conv(out)\n\n        # Global pooling\n        pool = self._global_pooling(x)\n        pool = self.global_pooling_conv(pool)\n        pool = self.global_pooling_bn(pool)  # Removed the stray | character\n        pool = self.pool_red_conv(pool)\n        if self.training or self.pooling_size is None:\n            pool = pool.repeat(1, 1, x.size(2), x.size(3))\n\n        out += pool\n        out = self.red_bn(out)\n        return out\n\n    def _global_pooling(self, x):\n        if self.training or self.pooling_size is None:\n            pool = x.view(x.size(0), x.size(1), -1).mean(dim=-1)\n            pool = pool.view(x.size(0), x.size(1), 1, 1)\n        else:\n            pooling_size = (min(try_index(self.pooling_size, 0), x.shape[2]),\n                          min(try_index(self.pooling_size, 1), x.shape[3]))\n            padding = (\n                (pooling_size[1] - 1) // 2,\n                (pooling_size[1] - 1) // 2 if pooling_size[1] % 2 == 1 else (pooling_size[1] - 1) // 2 + 1,\n                (pooling_size[0] - 1) // 2,\n                (pooling_size[0] - 1) // 2 if pooling_size[0] % 2 == 1 else (pooling_size[0] - 1) // 2 + 1\n            )\n\n            pool = functional.avg_pool2d(x, pooling_size, stride=1)\n            pool = functional.pad(pool, pad=padding, mode=\"replicate\")\n        return pool","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:17:00.045331Z","iopub.execute_input":"2025-04-28T20:17:00.045601Z","iopub.status.idle":"2025-04-28T20:17:00.067647Z","shell.execute_reply.started":"2025-04-28T20:17:00.045575Z","shell.execute_reply":"2025-04-28T20:17:00.066721Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"!pip install tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.950980Z","iopub.status.idle":"2025-04-28T20:06:45.951273Z","shell.execute_reply.started":"2025-04-28T20:06:45.951122Z","shell.execute_reply":"2025-04-28T20:06:45.951141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    import sys\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorboardX\"])\n    from tensorboardX import SummaryWriter\n\n\ntry:\n    from lib import dataset as D\n    from lib import train_utils as U\n    from lib.common import count_parameters, is_sorted, compute_mask_class, to_numpy\n    from lib.metrics import JaccardIndex, AverageMeter, PixelAccuracy, threshold_mining, do_kaggle_metric\n    from lib.train_utils import logit_to_prob\n    from test import generate_model_submission\nexcept ImportError as e:\n    print(f\"Error importing custom modules: {e}\")\n    print(\"Please ensure your custom modules are in the Python path\")\n\ntqdm.monitor_interval = 0  # Workaround for https://github.com/tqdm/tqdm/issues/481\n\ndef main():\n    # Rest of your code remains the same...\n    pass\n\nif __name__ == '__main__':\n    cudnn.benchmark = True\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:06:45.952179Z","iopub.status.idle":"2025-04-28T20:06:45.952426Z","shell.execute_reply.started":"2025-04-28T20:06:45.952306Z","shell.execute_reply":"2025-04-28T20:06:45.952316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}